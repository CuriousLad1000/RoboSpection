{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b54314-7d17-4209-92eb-ad83f44f59ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import warnings\n",
    "import pyaudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "warnings.filterwarnings('ignore', module='pyaudio')\n",
    "\n",
    "for index, name in enumerate(sr.Microphone.list_microphone_names()):\n",
    "    print(\"Microphone with name \\\"{1}\\\" found for `Microphone(device_index={0})`\".format(index, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39648f9e-7193-4e89-8705-823c34620baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kokoro import KPipeline\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "pipeline_TTS = KPipeline(lang_code='b',repo_id = 'hexgrad/Kokoro-82M')\n",
    "\n",
    "\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "import re\n",
    "from typing import List, Optional, Tuple\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "import rospy\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import sys\n",
    "import moveit_commander\n",
    "import moveit_msgs.msg\n",
    "from configparser import ConfigParser\n",
    "\n",
    "import pyautogui\n",
    "import threading\n",
    "\n",
    "\n",
    "from utils import CameraProcessor, PointCloudViewer, Inspector\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "config = ConfigParser() #For config.ini\n",
    "\n",
    "\n",
    "rospy.init_node(\"Cognitive_Motion\", anonymous=True)\n",
    "\n",
    "moveit_commander.roscpp_initialize(sys.argv)\n",
    "robot = moveit_commander.RobotCommander()\n",
    "scene = moveit_commander.PlanningSceneInterface()\n",
    "group_name = \"panda_manipulator\"\n",
    "move_group = moveit_commander.MoveGroupCommander(group_name) #we'll pass it on while calling functions\n",
    "\n",
    "\n",
    "\n",
    "zoom_def = 0.78\n",
    "front_def = [ -0.1202421863757063, -0.98750918258911113, -0.10182058199487866 ]\n",
    "lookat_def = [ 0.23267809182614518, 0.058752367596889288, 0.42837016799860811 ]\n",
    "up_def = [ -0.018570743687429749, -0.10030934011800337, 0.99478297319766518 ]\n",
    "\n",
    "\n",
    "saved_path = \"VI_appdata/Saved_coordinates/\"\n",
    "\n",
    "\n",
    "pose_list = ['initial_coordinates_down_med', 'initial_coordinates_down_high', 'initial_coordinates_front_low',\n",
    "             'initial_coordinates_front_med', 'initial_coordinates_left_origin_low', 'initial_coordinates_left_origin_med',\n",
    "             'initial_coordinates_left_extended_low', 'initial_coordinates_left_extended_med', 'initial_coordinates_right_origin_low',\n",
    "             'initial_coordinates_right_origin_med', 'initial_coordinates_right_extended_low', 'initial_coordinates_right_extended_med',\n",
    "             'initial_coordinates_current_pose']\n",
    "\n",
    "pose_list_joint = ['initial_coordinates_down_med_joint', 'initial_coordinates_down_high_joint', 'initial_coordinates_front_low_joint',\n",
    "                   'initial_coordinates_front_med_joint', 'initial_coordinates_left_origin_low_joint', 'initial_coordinates_left_origin_med_joint',\n",
    "                   'initial_coordinates_left_extended_low_joint', 'initial_coordinates_left_extended_med_joint', 'initial_coordinates_right_origin_low_joint',\n",
    "                   'initial_coordinates_right_origin_med_joint', 'initial_coordinates_right_extended_low_joint','initial_coordinates_right_extended_med_joint',\n",
    "                   'initial_coordinates_current_pose_joint']\n",
    "\n",
    "mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1,origin=[0, 0, 0])\n",
    "World_mesh_big = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3,origin=[0, 0, 0])\n",
    "\n",
    "config = ConfigParser() #recreate the object\n",
    "file_path = 'VI_appdata/'\n",
    "file_name = 'config.ini'\n",
    "\n",
    "\n",
    "if len(config.read(file_path+file_name)) < 1:\n",
    "    print(\"File not Found!\")\n",
    "else:\n",
    "    #Load all variables\n",
    "    samples = max(config.getint('Settings', 'samples'),1)\n",
    "    spacing = max(config.getfloat('Settings', 'spacing'),0)\n",
    "    offset_y = config.getfloat('Settings', 'offset_y')\n",
    "    offset_z = config.getfloat('Settings', 'offset_z')\n",
    "    trim_base = config.getfloat('Settings', 'trim_base')\n",
    "    manual_offset = config.getfloat('Settings', 'manual_offset')\n",
    "    cluster_centered = config.getboolean('Settings', 'Cluster_centered')\n",
    "    cluster_idx = config.getint('Settings', 'Cluster_idx')\n",
    "    cluster_discard = config.getint('Settings', 'Cluster_discard')\n",
    "    eps = config.getfloat('Settings', 'eps')\n",
    "    min_points = config.getint('Settings', 'min_points')\n",
    "    cluster_trim = config.getfloat('Settings', 'Cluster_trim')\n",
    "    tgt_coord_samples = max(config.getint('Settings', 'TGT_coord_Samples'),3)\n",
    "    tgt_final_trim = config.getfloat('Settings', 'TGT_final_trim')\n",
    "    tgt_reverse = config.getboolean('Settings', 'TGT_reverse')\n",
    "    tgt_preview = config.getboolean('Settings', 'TGT_preview')\n",
    "    z_offset = config.getfloat('Settings', 'z_offset')\n",
    "    coord_skip = max(config.getint('Settings', 'coord_skip'),0)+1 #+1 to match the for loop's skip method.\n",
    "    tgt_motion_delay = config.getfloat('Settings', 'TGT_motion_delay')\n",
    "    tgt_save = config.getboolean('Settings', 'TGT_save')\n",
    "    dbug = config.getboolean('Settings', 'Dbug')\n",
    "\n",
    "\n",
    "view_cam_parameters = \"VI_appdata/view.json\"\n",
    "\n",
    "\n",
    "camera_processor = CameraProcessor(samples=samples, offset_y=offset_y, offset_z=offset_z, trim_base=trim_base, manual_offset=manual_offset, \n",
    "                                   cluster_discard=cluster_discard, spacing=spacing, eps=eps, min_points=min_points, cluster_trim=cluster_trim, \n",
    "                                   tgt_coord_samples=tgt_coord_samples, tgt_final_trim=tgt_final_trim, tgt_reverse=tgt_reverse, tgt_preview=tgt_preview, \n",
    "                                   z_offset=z_offset, coord_skip=coord_skip, tgt_motion_delay=tgt_motion_delay, tgt_save=tgt_save, dbug=dbug, robo=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "selected_pose_joint = eval(config.get('Init_Pose', pose_list_joint[1])) #eval will revert char string to whatever it was.\n",
    "print(\"Moving to initial position\")\n",
    "camera_processor.go_to_joint_state(move_group, selected_pose_joint)\n",
    "\n",
    "###====================================================================================================================================================================\n",
    "\n",
    "def strip_code_block_old(text):\n",
    "    #Remove triple-backtick code blocks (with optional language like ```python)\n",
    "    text = re.sub(r\"```(?:\\w+)?\\n(.*?)```\", r\"\\1\", text, flags=re.DOTALL)\n",
    "\n",
    "    #Remove inline backtick-wrapped code like `print(\"Whatever!\")`\n",
    "    text = re.sub(r\"`([^`]*)`\", r\"\\1\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def strip_code_block(text):\n",
    "    \"\"\"Extracts Python code from a string wrapped in <code>...</code> tags.\"\"\"\n",
    "    match = re.search(r\"<code>(.*?)</code>\", text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else text.strip()\n",
    "\n",
    "\n",
    "def update_msg(msg: list, role = \"user\",content=\"\") -> list:\n",
    "    \"\"\"\n",
    "    Returns a new message list with an additional message dictionary appended.\n",
    "\n",
    "    Parameters:\n",
    "    - msg (list): The original list of message dictionaries (e.g., [{\"role\": ..., \"content\": ...}, ...]).\n",
    "    - role (str): The role for the new message (default is \"user\").\n",
    "    - content (str): The content of the new message.\n",
    "\n",
    "    Returns:\n",
    "    - list: A new list with the additional message appended, leaving the original list unchanged.\n",
    "    \"\"\"\n",
    "    tmp = list(msg)\n",
    "    template = {\"role\": role, \"content\": content}\n",
    "    tmp.append(template)\n",
    "    #print(msg)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "#============================================================================================================== To detect if generated response is code or text...\n",
    "PYTHON_KEYWORDS = [\"print(\", \"say(\", \"len(\", \"thread_handle.\"]\n",
    "\n",
    "def is_python_code(text, heuristic_threshold=1):\n",
    "    \n",
    "    lines = text.strip().split('\\n')\n",
    "    keyword_count = sum(any(kw in line for kw in PYTHON_KEYWORDS) for line in lines)\n",
    "    has_colon = any(line.strip().endswith(':') for line in lines)\n",
    "    has_equals = any('=' in line and '==' not in line for line in lines)\n",
    "    has_indent = any(line.startswith(('    ', '\\t')) for line in lines)\n",
    "    has_brackets = any(re.search(r'[\\[\\]\\(\\)\\{\\}]', line) for line in lines)\n",
    "    is_multiline = len(lines) > 1\n",
    "    score = (keyword_count + int(has_colon) + int(has_equals) + int(has_indent) + int(has_brackets) + int(is_multiline))\n",
    "    #print(f\"keyword_count = {keyword_count}, has_colon = {int(has_colon)}, has_equals = {int(has_equals)}, has_indent = {int(has_indent)}, is_multiline = {int(is_multiline)}\")\n",
    "    return score >= heuristic_threshold\n",
    "\n",
    "\n",
    "#Qwen/Qwen2.5-1.5B-Instruct\n",
    "#Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
    "#Qwen/Qwen2.5-Coder-3B-Instruct\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\n",
    "\n",
    "System_prompt_no_reason = \"\"\"Imagine we are working on HRC based path planning system using a manipulator robot. The robotic arm has a depth camera attached to its end effector. I would like you to assist me in interacting with the system and sending commands to this robot. \n",
    "There are five main steps that are required to complete the Inspection.\n",
    "STEP 1: Fetch point cloud\n",
    "STEP 2: Cluster point cloud\n",
    "STEP 3: Generate Inspection path\n",
    "STEP 4: Create robot targets\n",
    "STEP 5: Run through targets  OR   plan and execute path\n",
    "If the user asks what to do, briefly explain in few lines these steps on higher level without giving function names or generating any code. Do not tell how to call functions. Instead, tell user the simple commands that can be called to complete the steps.\n",
    "\n",
    "For example1:\n",
    "Me: Who are you?\n",
    "You: I am an AI assistant programmed to assist you with HRC based path planning system using a manipulator robot. \n",
    "\n",
    "For example2:\n",
    "Me: What should I do first?\n",
    "You: To get started with the HRC based path planning system using a manipulator robot, you can begin by loading the point cloud data from the depth camera by instructing me to `fetch point cloud`.\n",
    "\n",
    "For example3:\n",
    "Me: What should I do next?\n",
    "You: Next, you can cluster the point cloud into individual objects or points by instructing me to `cluster point cloud`. This step helps in identifying different Objects in the frame and help is selecting correct object for inspection.\n",
    "\n",
    "At any point, you have access to the following set of functions and coding blocks that starts with <code> tag and ends with </code> tag.\n",
    "All functions and codes are in Python language and thier use is explained in comment that starts with #. You are not to use any hypothetical functions. Do not include code comments in codes.\n",
    "\n",
    "\n",
    "<code>\n",
    "say(\"fetching pointcloud.\") #Notify the user of fetching point cloud.\n",
    "pointcloud = camera_processor.load_point_cloud(samples, offset_y, offset_z, manual_offset, spacing, trim_base, Hide_prev=False, Dbug=dbug, eval_tag=False) #This function when called with all parameters as listed, fetches a point cloud.\n",
    "</code>\n",
    "\n",
    "\n",
    "<code>\n",
    "thread_handle.exit_viewer() #This function when called, closes the opened point cloud viewer or selector. \n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.next_pc()     #This function when called, shows the next object or point cloud.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.prev_pc()         #This function when called, shows the previous object or point cloud.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.rotate_object()    #This function when called, applies rotation animation loop to the object or point cloud.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.stop_rotation() #This function when called, stops rotation of the object or point cloud.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.reset_view()  #This function when called, resets the view of the object or point cloud.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_current()  #This function when called, selects the current object or point cloud.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.deselect_current() #This function when called, deselects the current object or point cloud.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_centered_profile_around_y() #This function when called, selects the centered profile around y axis.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_centered_profile_around_x() #This function when called, selects the centered profile around x axis.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_centered_profile_around_z() #This function when called, selects the centered profile around z axis.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_specific_profile_around_y(p) #This function takes specific profile number p as input and returns that specific profile around y axis as output.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_specific_profile_around_x(p) #This function takes specific profile number p as input and returns that specific profile around x axis as output.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_specific_profile_around_z(p) #This function takes specific profile number p as input and returns that specific profile around z axis as output.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_multiple_profiles_around_y(n=k) #This function takes number of profiles k as input and returns that many profiles around y axis as output.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_multiple_profiles_around_x(n=k) #This function takes number of profiles k as input and returns that many profiles around x axis as output.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_multiple_profiles_around_z(n=k) #This function takes number of profiles k as input and returns that many profiles around z axis as output.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_profile_with_angle(angle_degrees=45, direction='left') #This function takes angle in degrees and direction 'left' or 'right' as input and returns single profile as output. Here, it selects a single profile 45 degrees towards left of the object.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.select_multiple_profiles_with_angle(angle_degrees=45, direction='right', n=10) #This function takes angle in degrees, direction 'left' or 'right' and number of profiles n as input and returns n number of profiles as output. Here, it selects 10 profiles 45 degrees towards right of the object.\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "thread_handle.preview_points_order(profiles, delay=0.05) #This function takes list of profiles and delay in seconds as input and shows a preview of profiles.\n",
    "</code>\n",
    "\n",
    "\n",
    "\n",
    "The following block of code that starts with <code> and ends with </code> tag clusters the point cloud.\n",
    "<code>\n",
    "#cluster pointcloud or object \n",
    "clouds = camera_processor.cluster_point_cloud(pointcloud, eps=eps, min_points=min_points) \n",
    "if cluster_discard > 0:\n",
    "    cld_idx_remove = []\n",
    "    for cld_idx in range (len(clouds)):\n",
    "        if len(np.array(clouds[cld_idx].points)) <= cluster_discard:\n",
    "            cld_idx_remove.append(cld_idx)\n",
    "    clouds = np.delete(clouds, cld_idx_remove, axis=0)\n",
    "\n",
    "if len(clouds)==1:\n",
    "    results = clouds[0]\n",
    "    say(\"Clustering completed. One object found!\")\n",
    "else:\n",
    "    thread_handle = None\n",
    "    def start_viewer():\n",
    "        global thread_handle\n",
    "        thread_handle = PointCloudViewer(clouds, results, say, view_cam_parameters)\n",
    "        thread_handle.run()\n",
    "    say(\"There are \" + str(len(clouds)) + \" objects available. Please select the desired objects to inspect!\")\n",
    "    results = []\n",
    "    thread = threading.Thread(target=start_viewer)\n",
    "    thread.start()\n",
    "</code>\n",
    "\n",
    "\n",
    "The following block of code that starts with <code> and ends with </code> tag generates inspection profiles.\n",
    "<code>\n",
    "#generate inspection profiles\n",
    "thread_handle = None\n",
    "profiles = []\n",
    "def start_inspector():\n",
    "    global thread_handle\n",
    "    thread_handle = Inspector(results, spacing, profiles, say, view_cam_parameters)\n",
    "    thread_handle.run()\n",
    "\n",
    "say(\"Please select the desired inspection paths around the selected object.\")\n",
    "say(\"You have the option to choose a path that passes through the center of the object around any axis. Alternatively, you can select a specific profile around an axis to serve as a reference for path generation. You can also direct me to inspect the object from any angle to the left or right.\")\n",
    "thread = threading.Thread(target=start_inspector)\n",
    "thread.start()\n",
    "</code>\n",
    "\n",
    "<code>\n",
    "say(\"creating robot targets\")\n",
    "Batch_Profiles = camera_processor.create_robot_targets(move_group, profiles) #This function when called with parameters as listed, creates robot targets. \n",
    "</code>\n",
    "\n",
    "The following block of code that starts with <code> and ends with </code> tag creates cartesian plan and execute.\n",
    "<code>\n",
    "say(\"planning\")\n",
    "res,fra = camera_processor.plan_cartesian_path(move_group, Batch_Profiles, eef_step=0.01, jump_threshold=0.0, velocity_scale=0.1, acceleration_scale=0.1)  \n",
    "say(f\"Planned {fra * 100:.2f}% of the path.\")\n",
    "say(\"Executing plan!\")\n",
    "camera_processor.execute_plan(move_group, res)\n",
    "</code>\n",
    "\n",
    "The following block of code that starts with <code> and ends with </code> tag moves the robot end effector through the targets gennerated using create_robot_targets() function.\n",
    "<code>\n",
    "#run through targets\n",
    "say(\"moving through targets.\")\n",
    "for coords in Batch_Profiles:\n",
    "    cam_tgt = coords[0]\n",
    "    eef_tgt = coords[1]\n",
    "    for id_x in range(0,len(eef_tgt)):  \n",
    "        camera_processor.publish_coordinates([cam_tgt[id_x]], \"world\", 'Camera_Target', static = False)   \n",
    "        camera_processor.go_to_coord_goal(move_group, eef_tgt[id_x])\n",
    "        print(\"Moving to Target:\",id_x+1)\n",
    "        time.sleep(tgt_motion_delay)\n",
    "</code>\n",
    "\n",
    "\n",
    "Note that when user gives specific commands like cluster point cloud you will have to generate that specific code block from provided section. Note that code sections start with <code> tag and ends with </code> tag. Also include variable that stores result. Do not include code comments in codes.\n",
    "\n",
    "All of your outputs need to be identified by one of the following tags: \n",
    "<code>Output code command that achieves the desired goal</code>\n",
    "\n",
    "For example1:\n",
    "Me: rotate object\n",
    "You:\n",
    "<code>\n",
    "thread_handle.rotate_object()\n",
    "</code>\n",
    "\n",
    "For example2:\n",
    "Me: select center profile around y axis\n",
    "You:\n",
    "<code>\n",
    "thread_handle.select_centered_profile_around_y()\n",
    "</code>\n",
    "\n",
    "For example3:\n",
    "Me: generate inspection profiles\n",
    "You:\n",
    "<code>\n",
    "thread_handle = None\n",
    "profiles = []\n",
    "def start_inspector():\n",
    "    global thread_handle\n",
    "    thread_handle = Inspector(results, spacing, profiles, say, view_cam_parameters)\n",
    "    thread_handle.run()\n",
    "\n",
    "say(\"Please select the desired inspection paths around the selected object.\")\n",
    "say(\"You have the option to choose a path that passes through the center of the object around any axis. Alternatively, you can select a specific profile around an axis to serve as a reference for path generation. You can also direct me to inspect the object from any angle to the left or right.\")\n",
    "thread = threading.Thread(target=start_inspector)\n",
    "thread.start()\n",
    "</code>\n",
    "\n",
    "Whenever user asks a question, then answer using say function.\n",
    "example1:\n",
    "Me: How many clusters or objects are available?\n",
    "You:\n",
    "<code>\n",
    "say(f\"There are {len(clouds)} clusters available\")\n",
    "</code>\n",
    "\n",
    "example2:\n",
    "Me: How many profiles are available around x axis?\n",
    "You:\n",
    "<code>\n",
    "say(f\"There are {thread_handle.profiles_available('x')} profiles around x axis available for selection.\") #use say() function to print number of profiles available around x axis.\n",
    "</code>\n",
    "\n",
    "example3:\n",
    "Me: How many profiles are available around x and y axis?\n",
    "You:\n",
    "<code>\n",
    "say(f\"There are {thread_handle.profiles_available('x')} profiles around x axis and {thread_handle.profiles_available('y')} around y axis available for selection.\")  #use say() function to print number of profiles available around x and y axis.\n",
    "</code>\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "#Template: List has dictionary values. Each dictionary has 2 sets of keys and values, keys: role and content, values like system, user etc.. and respective content.\n",
    "#msg[2]['role']\n",
    "\n",
    "msg_system = [{'role': 'system', 'content': System_prompt_no_reason}]\n",
    "\n",
    "\n",
    "#================================================================================================= TTS KOKORO\n",
    "\n",
    "def say(text):\n",
    "    TTS_generator = pipeline_TTS(text, voice='bf_emma', speed=1, split_pattern=r'\\n+')\n",
    "    \n",
    "    for i, (gs, ps, audio) in enumerate(TTS_generator):\n",
    "        #print(i)  #i => index\n",
    "        #print(gs) #gs => graphemes/text\n",
    "        #print(ps) #ps => phonemes\n",
    "        duration = len(audio) / 24000  #duration in seconds\n",
    "\n",
    "        display(Audio(data=audio, rate=24000, autoplay=True))\n",
    "        #display(Audio(data=audio, rate=24000, autoplay=i==0))\n",
    "        time.sleep(duration)\n",
    "        #sf.write(f'{i}.wav', audio, 24000) #save each audio file\n",
    "\n",
    "\n",
    "#============================================================================================  Speech Recognition\n",
    "\n",
    "\n",
    "WAKE_WORDS = [\"hey Franka\", \"Here Franco\", \"Here, Franco\"]\n",
    "EXIT_WORDS = [\"goodbye Franka\"]\n",
    "\n",
    "System_cmd = [\"System command\"]\n",
    "System_WORDS = [\"reload model\"]\n",
    "\n",
    "\n",
    "prompt_flag = False\n",
    "llm_flag=False\n",
    "system_flag = False\n",
    "reload_model = False\n",
    "Exit_speech = False  #Exit_speech flag for background listener.\n",
    "say_flag=False\n",
    "\n",
    "transcription = \"\"\n",
    "\n",
    "\n",
    "#Normalize and clean transcription\n",
    "def normalize_text(text: str) -> str:\n",
    "    return re.sub(r'[^a-z ]+', '', text.lower().strip())\n",
    "\n",
    "def is_similar_word(text: str, keywords: List[str], threshold: int = 80) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Check if text is similar to any keyword above a threshold.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to compare.\n",
    "        keywords (List[str]): List of wake words.\n",
    "        threshold (int, optional): Matching threshold. Defaults to 80.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[bool, Optional[str]]: (True, matching keyword) if found, otherwise (False, None).\n",
    "    \"\"\"\n",
    "    if not text or not keywords:\n",
    "        return False, None\n",
    "        \n",
    "    norm_text = normalize_text(text)\n",
    "\n",
    "    for keyword in keywords:\n",
    "        if fuzz.ratio(norm_text, normalize_text(keyword)) >= threshold:\n",
    "            return True, keyword\n",
    "\n",
    "    return False, None\n",
    "\n",
    "def listen_and_trigger(recognizer):\n",
    "    global mic2\n",
    "    #global transcription\n",
    "    global llm_flag\n",
    "    global prompt_flag\n",
    "    \n",
    "    with mic2 as source:\n",
    "        #recognizer.adjust_for_ambient_noise(source)\n",
    "        print(\"\\n[Trigger Listening...]\")\n",
    "        try:\n",
    "            audio = recognizer.listen(source, timeout=20)\n",
    "\n",
    "            #use internal faster whisper...\n",
    "            transcription = recognizer.recognize_faster_whisper(audio_data=audio, model=\"faster-whisper-small.en\", show_dict=False)\n",
    "            \n",
    "            #transcription = recognizer.recognize_google(audio)\n",
    "            print(f\"Trigger STT: {transcription}\")\n",
    "\n",
    "            llm_flag = True\n",
    "            print(\"llm_flag set\\n\")\n",
    "            prompt_flag = False\n",
    "            return transcription\n",
    "\n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"Timeout reached.\")\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Speech recognition error: {e}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#this is called from the background thread\n",
    "def callback_franka(recognizer, audio):\n",
    "    #received audio data, now we'll recognize it using Google Speech Recognition\n",
    "    global Exit_speech\n",
    "    #global transcription\n",
    "    global prompt_flag\n",
    "    global system_flag\n",
    "    global reload_model\n",
    "    global llm_flag\n",
    "    global say_flag\n",
    "    \n",
    "    #use internal faster whisper...\n",
    "    transcription = recognizer.recognize_faster_whisper(audio_data=audio, model=\"faster-whisper-small.en\", show_dict=False)\n",
    "    #transcription = recognizer.recognize_google(audio)\n",
    "    if len(transcription)== 0:\n",
    "        return\n",
    "        \n",
    "    print(f\"Background STT: {transcription}\")\n",
    "\n",
    "    if system_flag:\n",
    "        \n",
    "        cmd = is_similar_word(transcription, System_WORDS)[1]\n",
    "        \n",
    "        if cmd == \"reload model\":\n",
    "            print(f\"System cmd: {cmd}\")\n",
    "            reload_model = True\n",
    "        else:\n",
    "            print(\"Invalid System command.\")\n",
    "\n",
    "        system_flag = False\n",
    "        print(\"system_flag reset\",\"\\n\")\n",
    "\n",
    "    elif is_similar_word(transcription, System_cmd)[0]:\n",
    "        print(f\"System Wake word: {transcription}\")\n",
    "        system_flag = True\n",
    "        prompt_flag = False\n",
    "        say_flag = True\n",
    "        print(\"What is your command?\")\n",
    "        print()\n",
    "\n",
    "    elif is_similar_word(transcription, WAKE_WORDS)[0]:\n",
    "        print(f\"Wake word: {transcription}\")\n",
    "        prompt_flag = True\n",
    "        say_flag = True        \n",
    "\n",
    "    elif is_similar_word(transcription, EXIT_WORDS)[0]:\n",
    "        print(f\"Exit word: {transcription}\")\n",
    "        print(\"Exiting...\")\n",
    "        Exit_speech = True\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "mic = sr.Microphone()\n",
    "mic2 = sr.Microphone()\n",
    "\n",
    "with mic as source:\n",
    "    recognizer.adjust_for_ambient_noise(source)  #we only need to calibrate once, before we start listening\n",
    "\n",
    "print(\"Listening...\")\n",
    "#start listening in the background\n",
    "stop_listening = recognizer.listen_in_background(mic, callback_franka) #`stop_listening` is now a function that, when called, stops background listening\n",
    "\n",
    "\n",
    "#Do your stuff here in main thread...\n",
    "while True: \n",
    "\n",
    "    #Reload LLM model\n",
    "    if reload_model:\n",
    "        del tokenizer\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(5)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\n",
    "        \n",
    "        msg_system = [{'role': 'system', 'content': System_prompt_no_reason}]  #Reset context\n",
    "        \n",
    "        reload_model = False\n",
    "        print(\"model reloaded!\")\n",
    "        say(\"model reloaded!\")\n",
    "        print()\n",
    "\n",
    "    elif system_flag and say_flag:\n",
    "        say(\"what is your command?\")\n",
    "        say_flag=False\n",
    "        \n",
    "    elif prompt_flag and say_flag:\n",
    "        say(\"yes?\")\n",
    "        say_flag=False\n",
    "        prompt = listen_and_trigger(recognizer)\n",
    "\n",
    "    elif llm_flag:\n",
    "        if transcription != \"  You\": #Get rid of empty noise...\n",
    "            #prompt = transcription\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(\"Running LLM...\")\n",
    "\n",
    "            msg = update_msg(msg_system, role=\"user\", content = prompt)\n",
    "            inputs = tokenizer.apply_chat_template(msg, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "            outputs = model.generate(inputs, max_new_tokens=4096, do_sample=False, top_k=50, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "            response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True) \n",
    "            print(\"=========================== Response:\")\n",
    "            print(\"\\n\",response,\"\\n\")\n",
    "            print()\n",
    "            \n",
    "            #Feedback the generated response to model prompt to keep context. (will increase context size and VRAM usage...)\n",
    "            msg_system = update_msg(msg_system, role=\"assistant\", content = response)\n",
    "            \n",
    "            try:\n",
    "                code_block = strip_code_block(response)\n",
    "                if not (is_python_code(code_block)):\n",
    "                    code_block = f'say(\"{response}\")' #if text then speak...\n",
    "                    exec(code_block)\n",
    "                else:\n",
    "                    say(\"Okay!\")\n",
    "                    exec(code_block)\n",
    "                    say(\"done!\")\n",
    "            except Exception as e:\n",
    "                print(\"Error in executing command!\")\n",
    "                print(e)\n",
    "                say(\"Error in executing command!\")\n",
    "            \n",
    "        llm_flag = False\n",
    "        print(\"llm_flag reset\\n\")\n",
    "\n",
    "    \n",
    "    #Stop background listener, delete model... Exit_speech flag set by callback fuction.\n",
    "    elif Exit_speech == True:\n",
    "        say(\"Goodbye!\")\n",
    "        del tokenizer\n",
    "        del model\n",
    "        del pipeline_TTS\n",
    "        torch.cuda.empty_cache()\n",
    "        stop_listening(wait_for_stop=False)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759d9f6-eeae-42f4-8c8b-83b8653a14ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656af73f-10ab-4833-a757-5f201c9639a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "93d8f067-d36d-46ab-aae2-77b208f19279",
   "metadata": {},
   "source": [
    "STEP 1: Fetch point cloud\n",
    "\n",
    "STEP 2: Cluster point cloud\n",
    "\n",
    "STEP 3: Generate Inspection path\n",
    "\n",
    "STEP 4: Create robot targets\n",
    "\n",
    "STEP 5: Run through targets  OR   plan and execute path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (kokoro)",
   "language": "python",
   "name": "kokoro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
